{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f155d46a",
   "metadata": {},
   "source": [
    "# Data Management\n",
    "\n",
    "Welcome to the Data Management lab!\n",
    "\n",
    "In previous labs, you focused on building and training models with well-structured data. However, in the real world, data is rarely perfect. Even the most powerful model architecture can fail if it's fed a messy, inefficient, or unreliable data stream. This is where a robust **data pipeline** becomes essential.\n",
    "\n",
    "This lab shifts your focus from the model to the data itself, tackling the common challenges of real-world datasets. You'll be working with the **[Oxford 102 Flowers dataset](https://www.robots.ox.ac.uk/~vgg/data/flowers/102/)**, a collection of images and labels that are stored in separate files, with inconsistent formatting, and possibly even some corrupted samples. To overcome these hurdles, you will use PyTorch's core data management tools: The **`Dataset`** and **`DataLoader`** classes.\n",
    "\n",
    "In this lab, you will:\n",
    "\n",
    "* Explore a real-world dataset with unorganized files and separated labels.\n",
    "* Build a custom PyTorch **`Dataset`** to load and preprocess images and labels on-the-fly.\n",
    "* Apply **transformations** and **data augmentation** to prepare your data and improve model robustness.\n",
    "* Use the **`DataLoader`** to efficiently create and shuffle batches for training.\n",
    "* Split your data into training, validation, and test sets.\n",
    "* Implement error-handling techniques to manage data issues and monitor your pipeline’s performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4f33c0",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MJUe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import requests\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, Subset, random_split, DataLoader\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "import helper_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bkHC",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "## Data Access\n",
    "\n",
    "Every deep learning project starts with data, but not all data is ready to use.  \n",
    "Sometimes it’s already well organized, but often it’s scattered or stored in formats that aren’t directly compatible with model training.\n",
    "In this section, you’ll learn how to manage data access and follow best practices for working with unorganized or inconsistent datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162ba742",
   "metadata": {},
   "source": [
    "### Dataset Exploration\n",
    "\n",
    "\n",
    "The first step when working with a new dataset is to access and explore its structure.\n",
    "This helps you understand how the data is organized, and therefore, how to load and use it effectively.\n",
    "\n",
    "* You will define a  `download_dataset` function that downloads the dataset from a given URL and extracts it to a specified directory.\n",
    "* In this case, the dataset consists of two files: a `.tgz`  file with images and a `.mat` file containing the labels.\n",
    "\n",
    "Both files will be downloaded using the `requests` library and then extracted using the `tarfile` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35eab808",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def download_dataset():\n",
    "    \"\"\"\n",
    "    Downloads and extracts a dataset from remote URLs if not already present locally.\n",
    "\n",
    "    This function first checks for the existence of the dataset files in a specific\n",
    "    directory. If the files are not found, it proceeds to download them from\n",
    "    pre-defined URLs, showing progress bars, and then extracts the contents.\n",
    "    \"\"\"\n",
    "    # Define the directory to store the dataset.\n",
    "    data_dir = \"flower_data\"\n",
    "    \n",
    "    # Define paths for key files and folders.\n",
    "    image_folder_path = os.path.join(data_dir, \"jpg\")\n",
    "    labels_file_path = os.path.join(data_dir, \"imagelabels.mat\")\n",
    "    tgz_path = os.path.join(data_dir, \"102flowers.tgz\")\n",
    "\n",
    "    # Check if the primary data folder and a key label file already exist.\n",
    "    if os.path.exists(image_folder_path) and os.path.exists(labels_file_path):\n",
    "        # Inform the user that the dataset is already available locally.\n",
    "        print(f\"Dataset already exists. Loading locally from '{data_dir}'.\")\n",
    "        # Exit the function since no download is needed.\n",
    "        return\n",
    "\n",
    "    # Inform the user that the dataset is not found and the download will start.\n",
    "    print(\"Dataset not found locally. Downloading...\")\n",
    "\n",
    "    # Define the URLs for the image archive and the labels file.\n",
    "    image_url = \"https://www.robots.ox.ac.uk/~vgg/data/flowers/102/102flowers.tgz\"\n",
    "    labels_url = \"https://www.robots.ox.ac.uk/~vgg/data/flowers/102/imagelabels.mat\"\n",
    "\n",
    "    # Create the target directory for the dataset, if it doesn't already exist.\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "    # Announce the start of the image download process.\n",
    "    print(\"Downloading images...\")\n",
    "    # Send an HTTP GET request to the image URL, enabling streaming for large files.\n",
    "    response = requests.get(image_url, stream=True)\n",
    "    # Get the total size of the file from the response headers for the progress bar.\n",
    "    total_size = int(response.headers.get(\"content-length\", 0))\n",
    "\n",
    "    # Open a local file in binary write mode to save the downloaded archive.\n",
    "    with open(tgz_path, \"wb\") as file:\n",
    "        # Iterate over the response content in chunks with a progress bar.\n",
    "        for data in tqdm(\n",
    "            # Define the chunk size for iterating over the content.\n",
    "            response.iter_content(chunk_size=1024),\n",
    "            # Set the total for the progress bar based on the file size in kilobytes.\n",
    "            total=total_size // 1024,\n",
    "        ):\n",
    "            # Write each chunk of data to the file.\n",
    "            file.write(data)\n",
    "\n",
    "    # Announce the start of the file extraction process.\n",
    "    print(\"Extracting files...\")\n",
    "    # Open the downloaded tar.gz archive in read mode.\n",
    "    with tarfile.open(tgz_path, \"r:gz\") as tar:\n",
    "        # Extract all contents of the archive into the target directory.\n",
    "        tar.extractall(data_dir)\n",
    "\n",
    "    # Announce the start of the labels download process.\n",
    "    print(\"Downloading labels...\")\n",
    "    # Send an HTTP GET request to the labels URL.\n",
    "    response = requests.get(labels_url)\n",
    "    # Open a local file in binary write mode to save the labels.\n",
    "    with open(labels_file_path, \"wb\") as file:\n",
    "        # Write the entire content of the response to the file.\n",
    "        file.write(response.content)\n",
    "\n",
    "    # Inform the user that the download and extraction are complete.\n",
    "    print(f\"Dataset downloaded and extracted to '{data_dir}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "lEQa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset not found locally. Downloading...\n",
      "Downloading images...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tqdm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Call the function to download and prepare the dataset.\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mdownload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 44\u001b[39m, in \u001b[36mdownload_dataset\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m# Open a local file in binary write mode to save the downloaded archive.\u001b[39;00m\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(tgz_path, \u001b[33m\"\u001b[39m\u001b[33mwb\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[32m     43\u001b[39m     \u001b[38;5;66;03m# Iterate over the response content in chunks with a progress bar.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtqdm\u001b[49m(\n\u001b[32m     45\u001b[39m         \u001b[38;5;66;03m# Define the chunk size for iterating over the content.\u001b[39;00m\n\u001b[32m     46\u001b[39m         response.iter_content(chunk_size=\u001b[32m1024\u001b[39m),\n\u001b[32m     47\u001b[39m         \u001b[38;5;66;03m# Set the total for the progress bar based on the file size in kilobytes.\u001b[39;00m\n\u001b[32m     48\u001b[39m         total=total_size // \u001b[32m1024\u001b[39m,\n\u001b[32m     49\u001b[39m     ):\n\u001b[32m     50\u001b[39m         \u001b[38;5;66;03m# Write each chunk of data to the file.\u001b[39;00m\n\u001b[32m     51\u001b[39m         file.write(data)\n\u001b[32m     53\u001b[39m \u001b[38;5;66;03m# Announce the start of the file extraction process.\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'tqdm' is not defined"
     ]
    }
   ],
   "source": [
    "# Call the function to download and prepare the dataset.\n",
    "download_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PKri",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the path to the root directory of the dataset.\n",
    "path_dataset = './flower_data'\n",
    "\n",
    "# Display the folder structure of the dataset directory up to a depth of one.\n",
    "helper_utils.print_data_folder_structure(path_dataset, max_depth=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936a6408",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "You can see that the dataset consists of:\n",
    "- A  `jpg` folder containing images in JPEG format\n",
    "- An `imagelabels.mat` file that stores the labels (MATLAB format)\n",
    "- A `labels_description.txt` file describing each label\n",
    "\n",
    "This gives you a clear sense of how the dataset is structured and what you’ll be working with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bc471b",
   "metadata": {},
   "source": [
    "### Creating a Custom Dataset Class\n",
    "\n",
    "Having downloaded and briefly explored the dataset, the next step is to create a custom dataset class that organizes access to the data. The goal is to retrieve images and their corresponding labels consistently and efficiently for model training.\n",
    "\n",
    "You will create a class `FlowerDataset` that inherits from `torch.utils.data.Dataset`.\n",
    "A custom dataset class *must* implement the following methods:\n",
    "\n",
    "> `__init__`: Initializes the dataset object.\n",
    "> * Typically accepts the data path and any transforms to apply.\n",
    "> * Optionally loads metadata or labels associated with the data.\n",
    "\n",
    "> `__len__`: Returns the number of samples in the dataset.\n",
    "\n",
    "> `__getitem__`: Retrieves a single sample given an index.\n",
    "> * Loads the image and its label, applies transforms, and returns them.\n",
    "\n",
    "Additionally, you will define helper methods to make the code cleaner and more organized:\n",
    "\n",
    "> `load_and_correct_labels`: Loads image labels from the MATLAB .mat file.\n",
    "> * It uses `scipy.io.loadmat` to read the file and extract the labels array.\n",
    "> * The labels are adjusted by subtracting 1, which is necessary because MATLAB uses 1-based indexing, while Python uses 0-based indexing. \n",
    ">    * This correction prevents off-by-one errors during training and evaluation.\n",
    "\n",
    "> `retrieve_image`: Loads an image from disk given its index `idx`. \n",
    "> * Given `idx`, it constructs the filename and path, opens the image using the Pillow library, converts it to RGB format (to ensure consistency), and returns the image object.\n",
    "\n",
    "> `get_label_description`: Given a label, returns a human-readable description from the text file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5db148",
   "metadata": {},
   "source": [
    "**Regarding lazy loading**:\n",
    "The images are not loaded all at once when the dataset object is created.\n",
    "Instead, they are loaded on-the-fly when accessed via the `__getitem__` method.\n",
    "This approach is memory efficient, especially when dealing with large datasets, as it avoids loading all images into memory at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9993066",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FlowerDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A custom dataset class for loading flower image data.\n",
    "\n",
    "    This class is designed to work with PyTorch's Dataset and DataLoader\n",
    "    abstractions. It handles loading images and their corresponding labels\n",
    "    from a specific directory structure.\n",
    "    \"\"\"\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Initializes the dataset object.\n",
    "\n",
    "        Args:\n",
    "            root_dir (str): The root directory where the dataset is stored.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        # Store the root directory path.\n",
    "        self.root_dir = root_dir\n",
    "        # Store the optional transformations.\n",
    "        self.transform = transform\n",
    "        # Construct the full path to the image directory.\n",
    "        self.image_dir = os.path.join(self.root_dir, \"jpg\")\n",
    "        # Load and process the labels from the corresponding file.\n",
    "        self.labels = self.load_and_correct_labels()\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the total number of samples in the dataset.\n",
    "        \"\"\"\n",
    "        # The total number of samples is the number of labels.\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves a sample from the dataset at the specified index.\n",
    "\n",
    "        Args:\n",
    "            idx (int): The index of the sample to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing the image and its label.\n",
    "        \"\"\"\n",
    "        # Retrieve the image for the given index.\n",
    "        image = self.retrieve_image(idx)\n",
    "\n",
    "        # Check if a transform is provided.\n",
    "        if self.transform is not None:\n",
    "            # Apply the transform to the image.\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Get the label corresponding to the index.\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Return the processed image and its label.\n",
    "        return image, label\n",
    "\n",
    "    def retrieve_image(self, idx):\n",
    "        \"\"\"\n",
    "        Loads a single image from disk based on its index.\n",
    "\n",
    "        Args:\n",
    "            idx (int): The index of the image to load.\n",
    "\n",
    "        Returns:\n",
    "            PIL.Image.Image: The loaded image, converted to RGB.\n",
    "        \"\"\"\n",
    "        # Construct the image filename based on the index (e.g., 'image_00001.jpg').\n",
    "        img_name = f\"image_{idx + 1:05d}.jpg\"\n",
    "        # Construct the full path to the image file.\n",
    "        img_path = os.path.join(self.image_dir, img_name)\n",
    "        # Open the image file.\n",
    "        with Image.open(img_path) as img:\n",
    "            # Convert the image to the RGB color space and return it.\n",
    "            image = img.convert(\"RGB\")\n",
    "        return image\n",
    "\n",
    "    def load_and_correct_labels(self):\n",
    "        \"\"\"\n",
    "        Loads labels from a .mat file and adjusts them to be zero-indexed.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: An array of zero-indexed integer labels.\n",
    "        \"\"\"\n",
    "        # Load the MATLAB file containing the labels.\n",
    "        self.labels_mat = scipy.io.loadmat(\n",
    "            os.path.join(self.root_dir, \"imagelabels.mat\")\n",
    "        )\n",
    "        # Extract the labels array and correct for zero-based indexing.\n",
    "        labels = self.labels_mat[\"labels\"][0] - 1\n",
    "        # Return the processed labels.\n",
    "        return labels\n",
    "\n",
    "    def get_label_description(self, label):\n",
    "        \"\"\"\n",
    "        Retrieves the text description for a given label index.\n",
    "\n",
    "        Args:\n",
    "            label (int): The integer label.\n",
    "\n",
    "        Returns:\n",
    "            str: The corresponding text description of the label.\n",
    "        \"\"\"\n",
    "        # Construct the path to the file containing label descriptions.\n",
    "        path_labels_description = os.path.join(self.root_dir, \"labels_description.txt\")\n",
    "        # Open the label description file for reading.\n",
    "        with open(path_labels_description, \"r\") as f:\n",
    "            # Read all lines from the file.\n",
    "            lines = f.readlines()\n",
    "        # Get the description for the specified label and remove leading/trailing whitespace.\n",
    "        description = lines[label].strip()\n",
    "        # Return the clean description.\n",
    "        return description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SFPL",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize the dataset object, providing the path to the data.\n",
    "dataset = FlowerDataset(path_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947ee3d2",
   "metadata": {},
   "source": [
    "It is good practice to verify that your dataset class works as expected before using it to train a model.\n",
    "After creating an instance of the `FlowerDataset` class, you will:\n",
    "\n",
    "* Check the total number of samples in the dataset using the `len()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9581de1-01dd-40c1-abd7-a67f65dc775b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print the total number of samples in the dataset.\n",
    "print(f'Number of samples in the dataset: {len(dataset)}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e9da3a-355b-43e5-a7a9-4f482dd9ea5e",
   "metadata": {},
   "source": [
    "* Retrieve a specific sample using an index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c014c53-1e9e-4c08-97d4-d8440be935bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define an index for a sample to retrieve.\n",
    "sel_idx = 10\n",
    "\n",
    "# Retrieve the image and label for the selected index.\n",
    "img, label = dataset[sel_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19679c19-435a-444d-b83f-c37b445e0542",
   "metadata": {},
   "source": [
    "- Inspect the image size and label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RGSE",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a string detailing the image's dimensions.\n",
    "img_size_info = f\"Image size: {img.size}\"\n",
    "\n",
    "# Print the image size information along with its corresponding label.\n",
    "print(f'{img_size_info}, Label: {label}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52090766-f307-4844-88b2-9e88ac29b78a",
   "metadata": {},
   "source": [
    "* Visualize the image using the helper function `plot_img`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24967fd9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "helper_utils.plot_img(img, label=label, info=img_size_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335f776b",
   "metadata": {},
   "source": [
    "<br >\n",
    "\n",
    "Next, inspect the labels in the dataset. For each unique label, print its corresponding description using the `get_label_description` method.\n",
    "\n",
    "How are the labels?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53d6fdc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get all labels from the dataset object.\n",
    "dataset_labels = dataset.labels\n",
    "\n",
    "# Create a set of unique labels to remove duplicates.\n",
    "unique_labels = set(dataset_labels)\n",
    "\n",
    "# Iterate through each unique label.\n",
    "for label in unique_labels:\n",
    "    # Print the numerical label and its corresponding text description.\n",
    "    print(f'Label: {label}, Description: {dataset.get_label_description(label)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b167e8",
   "metadata": {},
   "source": [
    "### Overview of the Images in the Dataset\n",
    "\n",
    "\n",
    "Now that you have a working dataset class, you can explore the images in the dataset. You will make use of the `visual_exploration` function to visualize a few images to get a better understanding of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c87ab4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def visual_exploration(dataset, num_rows=2, num_cols=4):\n",
    "    \"\"\"\n",
    "    Displays a grid of randomly selected samples from a dataset for visual inspection.\n",
    "\n",
    "    Args:\n",
    "        dataset: The dataset object from which to draw samples. It should support\n",
    "                 indexing and have a `get_label_description` method.\n",
    "        num_rows (int): The number of rows in the display grid.\n",
    "        num_cols (int): The number of columns in the display grid.\n",
    "    \"\"\"\n",
    "    # Calculate the total number of images to display in the grid.\n",
    "    total_samples = num_rows * num_cols\n",
    "\n",
    "    # Select a random set of unique indices from the dataset.\n",
    "    indices = np.random.choice(len(dataset), total_samples, replace=False)\n",
    "\n",
    "    # Create a grid of subplots to hold the images.\n",
    "    fig, axes = helper_utils.get_grid(num_rows, num_cols, figsize=(num_cols * 3, num_rows * 4))\n",
    "\n",
    "    # Iterate over each subplot axis and the corresponding random sample index.\n",
    "    for ax, idx in zip(axes.flatten(), indices):\n",
    "        # Retrieve the image and its numerical label from the dataset.\n",
    "        image, label = dataset[idx]\n",
    "\n",
    "        # Get the human-readable text description for the label.\n",
    "        description = dataset.get_label_description(label)\n",
    "\n",
    "        # Format a new label string that includes both the number and description.\n",
    "        label = f\"{label} - {description}\"\n",
    "\n",
    "        # Create an information string with the sample's index and image dimensions.\n",
    "        info = f\"Index: {idx} Size: {image.size}\"\n",
    "\n",
    "        # Plot the image on the current subplot with its label and info.\n",
    "        helper_utils.plot_img(image, label=label, info=info, ax=ax)\n",
    "\n",
    "    # Render and display the entire grid of images.\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ROlb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Display a 2x4 grid of random samples from the dataset for visual inspection.\n",
    "visual_exploration(dataset, num_rows=2, num_cols=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZHCJ",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "## Quality Problems\n",
    "\n",
    "As you have seen in the previous section, the size of the images in the dataset varies significantly. This can be a problem when training a model, as most models expect input images to have the same size. To address this issue, you will implement a series of transformations to standardize the size of the images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e96758",
   "metadata": {},
   "source": [
    "### Transformations\n",
    "\n",
    "After exploring the dataset and identifying potential quality issues, the next step is to apply transformations to preprocess the images.\n",
    "Transformations are operations that modify images to prepare them for model training. Common examples include resizing, cropping, flipping, and normalizing pixel values.\n",
    "The `torchvision.transforms` module provides a variety of pre-defined transformations that can be easily applied to images.\n",
    "\n",
    "You’ll start by defining a simple transformation pipeline composed of two stages:\n",
    "\n",
    "Transformations **applied directly to the raw images**:\n",
    "- `Resize((256, 256))`: Resizes each image to a fixed size of 256×256 pixels.\n",
    "- `CenterCrop(224)`: Crops the center of the image to 224×224 pixels.\n",
    "\n",
    "Transformations that **convert and standardize images**:\n",
    "- `ToTensor()`: converts a PIL Image or NumPy array to a tensor and scales pixel values to the range [0, 1].\n",
    "- `Normalize(mean, std)`: normalizes the tensor using the specified mean and standard deviation.\n",
    "\n",
    "**Note**: \n",
    "The order of transformations often matters. Resizing and cropping should be applied before converting the image to a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de44914-8d0d-4395-b550-100e52aa7f6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the mean values for normalization.\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "# Define the standard deviation values for normalization.\n",
    "std = [0.229, 0.224, 0.225]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26cb1cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    # images transforms\n",
    "    transforms.Resize((256, 256)),  # Resize images to 256x256 pixels\n",
    "    transforms.CenterCrop(224),  # Center crop to 224x224 pixels\n",
    "    # bridge to tensor\n",
    "    transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
    "    # tensor transforms\n",
    "    transforms.Normalize(mean=mean, std=std),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b468748",
   "metadata": {},
   "source": [
    "You will now create a new instance of the `FlowerDataset` class, this time passing the transformation pipeline `transform` as an argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qnkX",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a new dataset instance with the specified image transformations.\n",
    "dataset_transformed = FlowerDataset(path_dataset, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af03786",
   "metadata": {},
   "source": [
    "Inspect the same sample again to see the effect of the transformations. Using `quick_debug`, you can see that the image has been resized and cropped to 224x224 pixels, and its pixel values have been normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TqIu",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Retrieve the transformed image and its label using the same index.\n",
    "img_transformed, label = dataset_transformed[sel_idx]\n",
    "\n",
    "# quick check\n",
    "helper_utils.quick_debug(img_transformed)\n",
    "\n",
    "# Plot the transformed image\n",
    "helper_utils.plot_img(img_transformed, label=label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0007f48",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Observe that the image is now a tensor with shape `[3, 224, 224]` representing 3 color channels (RGB) and 224×224 pixels.\n",
    "Because the pixel values have been normalized, they are no longer in the original range of [0, 255].\n",
    "\n",
    "That’s why the image looks different when visualized directly.\n",
    "To display it correctly, you need to *denormalize* it first. You can do this by applying the `Normalize` transformation again, using `new_mean` and `new_std` values to reverse the normalization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020c6df9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Denormalize:\n",
    "    \"\"\"\n",
    "    A callable class to reverse the normalization of a tensor image.\n",
    "\n",
    "    This class calculates the inverse transformation of a standard normalization\n",
    "    and can be used as a transform step, for instance, to visualize images\n",
    "    after they have been normalized for a model.\n",
    "    \"\"\"\n",
    "    def __init__(self, mean, std):\n",
    "        \"\"\"\n",
    "        Initializes the denormalization transform.\n",
    "\n",
    "        Args:\n",
    "            mean (list or tuple): The mean values used for the original normalization.\n",
    "            std (list or tuple): The standard deviation values used for the original\n",
    "                                 normalization.\n",
    "        \"\"\"\n",
    "        # Calculate the adjusted mean for the denormalization process.\n",
    "        new_mean = [-m / s for m, s in zip(mean, std)]\n",
    "        # Calculate the adjusted standard deviation for the denormalization process.\n",
    "        new_std = [1 / s for s in std]\n",
    "        # Create a Normalize transform object with the inverse parameters.\n",
    "        self.denormalize = transforms.Normalize(mean=new_mean, std=new_std)\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        \"\"\"\n",
    "        Applies the denormalization transform to a tensor.\n",
    "\n",
    "        Args:\n",
    "            tensor: The normalized tensor to be denormalized.\n",
    "\n",
    "        Returns:\n",
    "            The denormalized tensor.\n",
    "        \"\"\"\n",
    "        # Apply the denormalization transform to the input tensor.\n",
    "        return self.denormalize(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53b9c64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create an instance of the Denormalize class with the original mean and std.\n",
    "denormalize = Denormalize(mean=mean, std=std)\n",
    "# Apply the denormalization transform to the image tensor.\n",
    "img_tensor = denormalize(img_transformed)\n",
    "\n",
    "# Create an information string with the tensor's shape.\n",
    "img_shape_info = f\"Image Shape: {img_tensor.size()}\"\n",
    "# Plot the denormalized image to visualize the result.\n",
    "helper_utils.plot_img(img_tensor, label=label, info=img_shape_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993bb3e2",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "\n",
    "In this section, you’ll learn how to prepare and serve data efficiently for model training in PyTorch.\n",
    "A full training process typically includes three stages: training, validation, and evaluation.\n",
    "\n",
    "During training, the model learns from the training data by adjusting its weights based on the loss function. \n",
    "During validation, the model is evaluated on a separate dataset to tune hyperparameters and prevent overfitting.\n",
    "During evaluation, the model’s performance is tested on unseen data to assess its generalization ability.\n",
    "\n",
    "Each of these stages requires splitting the dataset into distinct subsets and using data loaders to feed data efficiently during training and testing. In what follows, you will see how to do this using PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a65e11",
   "metadata": {},
   "source": [
    "### Splitting the Dataset\n",
    "\n",
    "The `split_dataset` function will divide the dataset into training, validation, and test sets.\n",
    "To do this, you will:\n",
    "\n",
    "* Define the sizes of each subset from the given fractions for validation and test,\n",
    "* Make use of `random_split` from `torch.utils.data` to perform the actual split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ee3ebe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_dataset(dataset, val_fraction=0.15, test_fraction=0.15):\n",
    "    \"\"\"\n",
    "    Split the dataset into training, validation, and test sets.\n",
    "    \n",
    "    By default, this function splits the data into 70% for training,\n",
    "    15% for validation, and 15% for testing.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate the sizes of each split.\n",
    "    total_size = len(dataset)\n",
    "    val_size = int(total_size * val_fraction)\n",
    "    test_size = int(total_size * test_fraction)\n",
    "    train_size = total_size - val_size - test_size\n",
    "\n",
    "    # Use random_split to create the datasets.\n",
    "    train_dataset, val_dataset, test_dataset = random_split(\n",
    "        dataset, [train_size, val_size, test_size]\n",
    "    )\n",
    "    return train_dataset, val_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DnEU",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset, val_dataset, test_dataset = split_dataset(dataset_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d423a2da",
   "metadata": {},
   "source": [
    "Check the sizes of the resulting datasets to ensure they match the expected proportions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ulZA",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Length of training dataset:   {len(train_dataset)}\")\n",
    "print(f\"Length of validation dataset: {len(val_dataset)}\")\n",
    "print(f\"Length of test dataset:       {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZBYS",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "### Dataloaders\n",
    "\n",
    "Once your dataset is split into training, validation, and test sets, the next step is to serve data efficiently to your model during training and evaluation. \n",
    "PyTorch’s `DataLoader` class handles this by batching samples, shuffling data during training, and simplifying iteration over your dataset.\n",
    "Using dataloaders helps speed up training and ensures your model sees a diverse mix of samples in each batch.\n",
    "Now you will create dataloaders for each subset of the data (training, validation, and test).\n",
    "You will use the `DataLoader` class from `torch.utils.data` to create dataloaders.\n",
    "The arguments you will provide include:\n",
    "- `dataset`: The dataset to load data from (e.g., `train_dataset`, `val_dataset`, `test_dataset`),\n",
    "- `batch_size`: The number of samples per batch to load (e.g., `32`),\n",
    "- `shuffle`: Whether to shuffle the data at every epoch (typically `True` for training and `False` for validation and test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aLJB",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the batch size for the data loaders.\n",
    "batch_size = 32\n",
    "\n",
    "# Create the DataLoader for the training set, with shuffling enabled.\n",
    "train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Create the DataLoader for the validation set, with shuffling disabled.\n",
    "val_dataloader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Create the DataLoader for the test set, with shuffling disabled.\n",
    "test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0cd8b0",
   "metadata": {},
   "source": [
    "To understand how dataloaders work, you can iterate for two epochs over the training and validation dataloaders and, at the very end, iterate over the test dataloader.\n",
    "This will give you a sense of how data is served in batches during training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1378e58-1aac-414d-afc1-78d775afc33e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the total number of training epochs.\n",
    "n_epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fa979c-8c2a-4966-ac58-1841bff695a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Start the main training loop for each epoch.\n",
    "for epoch in range(n_epochs):\n",
    "    # Print a header to indicate the start of a new epoch.\n",
    "    print(f\"=== Processing epoch {epoch} ===\")\n",
    "\n",
    "    # Announce the start of the training phase.\n",
    "    print(f\"Pass number {epoch} through the training set\")\n",
    "    print('Training...')\n",
    "    # Get the total number of samples in the training set.\n",
    "    train_samples = len(train_dataset)\n",
    "    # Create a progress bar for the training data loader.\n",
    "    train_bar = helper_utils.get_dataloader_bar(train_dataloader, color='blue')\n",
    "    \n",
    "    # Iterate over the training data loader to get batches of images and labels.\n",
    "    for batch, (images, labels) in enumerate(train_dataloader):\n",
    "        # Update the training progress bar for the current batch.\n",
    "        helper_utils.update_dataloader_bar(train_bar, batch, batch_size, train_samples)\n",
    "\n",
    "    # Announce the start of the validation phase.\n",
    "    print(f\"\\nPass number {epoch} through the validation set\")\n",
    "    print('Validation...')\n",
    "    # Create a progress bar for the validation data loader.\n",
    "    val_bar = helper_utils.get_dataloader_bar(val_dataloader, color='orange')\n",
    "    # Get the total number of samples in the validation set.\n",
    "    val_samples = len(val_dataset)\n",
    "    \n",
    "    # Iterate over the validation data loader to get batches of images and labels.\n",
    "    for batch, (images, labels) in enumerate(val_dataloader):\n",
    "        # Update the validation progress bar for the current batch.\n",
    "        helper_utils.update_dataloader_bar(val_bar, batch, batch_size, val_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb0c9eb-f788-42bc-95e5-3bfd60e97a47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Announce the final evaluation on the test set.\n",
    "print(\"\\nFinal pass through the test set for evaluation\")\n",
    "# Create a progress bar for the test data loader.\n",
    "test_bar = helper_utils.get_dataloader_bar(test_dataloader, color='green')\n",
    "# Get the total number of samples in the test set.\n",
    "test_samples = len(test_dataset)\n",
    "\n",
    "# Iterate over the test data loader to get batches of images and labels.\n",
    "for batch, (images, labels) in enumerate(test_dataloader):\n",
    "    # Update the test progress bar for the current batch.\n",
    "    helper_utils.update_dataloader_bar(test_bar, batch, batch_size, test_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f40ced",
   "metadata": {},
   "source": [
    "<br>\n",
    "You can observe that the training dataloader has a total of 180 batches (for a total of 5733 samples). The validation and test dataloaders each have 39 batches (for a total of 1228 samples each)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Kclp",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "## Augmentation\n",
    "\n",
    "\n",
    "Data augmentation is an important technique for improving the robustness and generalization of deep learning models.\n",
    "By applying random transformations, such as flipping, rotating, or adjusting brightness, to training images, augmentation helps models recognize objects under varied real-world conditions. \n",
    "\n",
    "In PyTorch, augmentation can be performed \"on-the-fly,\" generating endless image variations without extra storage. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f7c399",
   "metadata": {},
   "source": [
    "Within `get_augmentation_transform`, you’ll create a more advanced transformation pipeline that includes data augmentation techniques.\n",
    "This pipeline consists of the following:\n",
    "\n",
    "Transformations that augment the raw images:\n",
    "- `RandomHorizontalFlip(p=0.5)`: This transformation randomly flips the image horizontally with a probability of 0.5,\n",
    "- `RandomRotation(degrees=10)`: This transformation randomly rotates the image within a range of -10 to +10 degrees,\n",
    "- `ColorJitter(brightness=0.2)`: This transformation randomly changes the brightness, contrast, saturation, and hue of the image.\n",
    "\n",
    "Transformations that standardize the images as `transform`: `Resize`, `CenterCrop`, `ToTensor`, and `Normalize`.\n",
    "\n",
    "You then create a new instance of the `FlowerDataset` class, this time passing the augmentation transformation pipeline `augmentation_transform` as an argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a4c77f-5095-4c3a-9f24-f549a4a04b28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_augmentation_transform(mean, std):\n",
    "    \"\"\"\n",
    "    Creates and returns a composition of image transformations for data augmentation\n",
    "    and preprocessing.\n",
    "\n",
    "    Args:\n",
    "        mean (list or tuple): A sequence of mean values for each channel.\n",
    "        std (list or tuple): A sequence of standard deviation values for each channel.\n",
    "\n",
    "    Returns:\n",
    "        torchvision.transforms.Compose: A composed pipeline of transformations.\n",
    "    \"\"\"\n",
    "    # Define a list of data augmentation transformations to be applied randomly.\n",
    "    augmentations_transforms = [\n",
    "        # Randomly flip the image horizontally with a 50% probability.\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        # Randomly rotate the image within a range of +/- 10 degrees.\n",
    "        transforms.RandomRotation(degrees=10),\n",
    "        # Randomly adjust the brightness of the image.\n",
    "        transforms.ColorJitter(brightness=0.2),\n",
    "    ]\n",
    "    \n",
    "    # Define the main list of standard, non-random transformations.\n",
    "    main_transforms = [\n",
    "        # Resize the input image to 256x256 pixels.\n",
    "        transforms.Resize((256, 256)),\n",
    "        # Crop the center 224x224 pixels of the image.\n",
    "        transforms.CenterCrop(224),\n",
    "        # Convert the PIL Image to a PyTorch tensor.\n",
    "        transforms.ToTensor(),\n",
    "        # Normalize the tensor with the provided mean and standard deviation.\n",
    "        transforms.Normalize(mean=mean, std=std),\n",
    "    ]\n",
    "\n",
    "    # Combine the augmentation and main transformations into a single pipeline.\n",
    "    transform = transforms.Compose(augmentations_transforms + main_transforms)\n",
    "    # Return the final composed transform object.\n",
    "    return transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfG",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the augmentation and preprocessing pipeline, providing the normalization stats.\n",
    "augmentation_transform = get_augmentation_transform(mean=mean, std=std)\n",
    "\n",
    "# Initialize a new dataset instance that will use the augmentation pipeline.\n",
    "dataset_augmented = FlowerDataset(path_dataset, transform=augmentation_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9d27f1",
   "metadata": {},
   "source": [
    "Debugging a dataset pipeline is a relevant step to ensure that the transformations and data loading processes are functioning as intended.\n",
    "`visualize_augmentation` allows you to visualize the effect of the augmentation transformations on a sample image. \n",
    "\n",
    "For a given number of versions, it retrieves the same sample image from the dataset. As the dataset applies random transformations, each retrieved version of the image will be affected differently, allowing you to see the variety of augmentations applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53dbe419",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def visualize_augmentations(dataset_aug, idx=0, num_versions=8):\n",
    "    \"\"\"\n",
    "    Displays multiple augmented versions of a single image from a dataset.\n",
    "\n",
    "    This function repeatedly retrieves an image from a dataset with augmentations\n",
    "    enabled, then displays each unique, randomly generated version in a grid\n",
    "    to help visualize the effect of the transformations.\n",
    "\n",
    "    Args:\n",
    "        dataset_aug: The dataset object with augmentation transforms applied.\n",
    "        idx (int): The index of the image in the dataset to visualize.\n",
    "        num_versions (int): The total number of augmented versions to display.\n",
    "    \"\"\"\n",
    "    # Create a denormalization transform to revert normalization for display.\n",
    "    denormalize = Denormalize(mean, std)\n",
    "\n",
    "    # Set the number of rows for the visualization grid.\n",
    "    n_rows = 2\n",
    "    # Calculate the number of columns needed based on the total versions to show.\n",
    "    n_cols = num_versions // n_rows\n",
    "    # Create a grid of subplots to display the images.\n",
    "    fig, axes = helper_utils.get_grid(n_rows, n_cols, figsize=(16, 8))\n",
    "\n",
    "    # Iterate through each subplot axis in the grid.\n",
    "    for ax in axes.flatten():\n",
    "        # Get a new, randomly augmented version of the same image by index.\n",
    "        img, label = dataset_aug[idx]\n",
    "\n",
    "        # Denormalize the image tensor so it can be displayed correctly.\n",
    "        img = denormalize(img)\n",
    "\n",
    "        # Plot the augmented image on the current subplot.\n",
    "        helper_utils.plot_img(img=img, ax=ax)\n",
    "\n",
    "    # Display the complete grid of augmented images.\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57057543",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Display 8 augmented versions of the selected image to see the transformations.\n",
    "visualize_augmentations(dataset_augmented, idx=sel_idx, num_versions=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df07efdb",
   "metadata": {},
   "source": [
    "### Splitting with Augmentation\n",
    "\n",
    "Integrating proper random splitting with data augmentation requires careful handling to ensure that the training, validation, and test sets are distinct and that augmentation is applied only to the training data.\n",
    "\n",
    "One subtle point to consider is that when using `random_split`, each of the splits is a `Subset` object that references the original dataset.\n",
    "Therefore those subsets will inherit the transformations defined in the original dataset and can not be assigned different transformations directly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adc39a9",
   "metadata": {},
   "source": [
    "To address this, you will create a custom dataset class `SubsetWithTransform` that wraps around a `Subset` and allows you to specify a different transformation for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5d18c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SubsetWithTransform(Dataset):\n",
    "    \"\"\"\n",
    "    A wrapper for a PyTorch Subset that applies a specific transformation.\n",
    "\n",
    "    This class allows for applying a different set of transformations to a\n",
    "    subset of a dataset, which is useful for creating distinct training,\n",
    "    validation, or test sets with different preprocessing steps from the\n",
    "    same base dataset.\n",
    "    \"\"\"\n",
    "    def __init__(self, subset, transform=None):\n",
    "        \"\"\"\n",
    "        Initializes the SubsetWithTransform object.\n",
    "\n",
    "        Args:\n",
    "            subset: A PyTorch Subset object containing a portion of a dataset.\n",
    "            transform (callable, optional): An optional transform to be applied\n",
    "                to the samples within this subset.\n",
    "        \"\"\"\n",
    "        # Store the original subset of the dataset.\n",
    "        self.subset = subset\n",
    "        # Store the transformations to be applied.\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the total number of samples in the subset.\n",
    "        \"\"\"\n",
    "        # Return the length of the underlying subset.\n",
    "        return len(self.subset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves a sample and applies the transform.\n",
    "\n",
    "        Args:\n",
    "            idx (int): The index of the sample to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing the transformed image and its label.\n",
    "        \"\"\"\n",
    "        # Get the original image and label from the underlying subset.\n",
    "        image, label = self.subset[idx]\n",
    "        # Check if a transform has been provided.\n",
    "        if self.transform:\n",
    "            # Apply the transform to the image.\n",
    "            image = self.transform(image)\n",
    "        # Return the transformed image and its label.\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5c05a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Apply the augmentation pipeline to the training subset.\n",
    "train_dataset = SubsetWithTransform(train_dataset, transform=augmentation_transform)\n",
    "# Apply the basic preprocessing transform to the validation subset.\n",
    "val_dataset = SubsetWithTransform(val_dataset, transform=transform)\n",
    "# Apply the basic preprocessing transform to the test subset.\n",
    "test_dataset = SubsetWithTransform(test_dataset, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8540beed",
   "metadata": {},
   "source": [
    "You can check that indeed the training dataset has the augmentation transformations applied, while the validation and test datasets do not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837f4612",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(train_dataset.transform)\n",
    "print(val_dataset.transform)\n",
    "print(test_dataset.transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3066a259",
   "metadata": {},
   "source": [
    "## Robust Datasets\n",
    "\n",
    "\n",
    "In real-world projects, data pipelines must be resilient to unexpected issues such as corrupted files, inconsistent image formats, or problematic samples that can crash training runs. \n",
    "This section introduces robust dataset design strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11efc00",
   "metadata": {},
   "source": [
    "### Error Handling\n",
    "\n",
    "\n",
    "You will create a new custom dataset class `RobustFlowerDataset` that extends the original `FlowerDataset` class to handle potential issues with corrupted or problematic images.\n",
    "\n",
    "The `__init__`, `__len__` and `load_and_correct_labels` methods remain mostly the same as in the original `FlowerDataset` class.\n",
    "\n",
    "The main differences are:\n",
    "\n",
    "> `__getitem__`: Implements a try-except block to catch exceptions that may occur when loading an image.\n",
    "> * If an exception occurs (e.g., due to a corrupted image file), the code will print and log the error by calling the `log_error` method. \n",
    "> * It then attempts to retrieve the next image in the dataset instead of stopping execution.\n",
    "\n",
    "> `retrieve_image`: Handles the actual image loading process. \n",
    "> * Constructs the filename and path, verifies image integrity using Pillow’s `verify()` method, and reloads the image to ensure it is fully loaded into memory.\n",
    "> * Checks the image size, raising an error if it’s smaller than 32 pixels in either dimension, and converts grayscale images to RGB for consistency.\n",
    "\n",
    "> `get_error_summary`: Provides a summary of all errors encountered during data loading, useful for debugging and assessing dataset quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfadba5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RobustFlowerDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A custom dataset class with robust error handling for loading images.\n",
    "\n",
    "    This class is designed to gracefully handle issues with individual data\n",
    "    samples, such as corrupted files or incorrect formats. It logs any errors\n",
    "    and attempts to load a different sample instead of crashing.\n",
    "    \"\"\"\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Initializes the dataset object.\n",
    "\n",
    "        Args:\n",
    "            root_dir (str): The root directory where the dataset is stored.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        # Store the root directory path.\n",
    "        self.root_dir = root_dir\n",
    "        # Construct the full path to the image directory.\n",
    "        self.img_dir = os.path.join(root_dir, \"jpg\")\n",
    "        # Store the optional transformations.\n",
    "        self.transform = transform\n",
    "        # Load and process the labels from the corresponding file.\n",
    "        self.labels = self.load_and_correct_labels()\n",
    "        # Initialize a list to keep track of any errors encountered.\n",
    "        self.error_logs = []\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves a sample, handling errors by trying the next available item.\n",
    "\n",
    "        Args:\n",
    "            idx (int): The index of the sample to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing the image and its label.\n",
    "        \"\"\"\n",
    "        # Loop to attempt loading a valid sample, preventing an infinite loop.\n",
    "        for attempt in range(len(self)):\n",
    "            # Attempt to load and process the sample.\n",
    "            try:\n",
    "                # Retrieve the image using the helper method.\n",
    "                image = self.retrieve_image(idx)\n",
    "                # Check if a transform has been provided.\n",
    "                if self.transform:\n",
    "                    # Apply the transform to the image.\n",
    "                    image = self.transform(image)\n",
    "                # Get the label for the current index.\n",
    "                label = self.labels[idx]\n",
    "                # Return the valid image and its corresponding label.\n",
    "                return image, label\n",
    "            # Catch any exception that occurs during the process.\n",
    "            except Exception as e:\n",
    "                # Log the error with its index and message.\n",
    "                self.log_error(idx, e)\n",
    "                # Move to the next index, wrapping around if necessary.\n",
    "                idx = (idx + 1) % len(self)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the total number of samples in the dataset.\n",
    "        \"\"\"\n",
    "        # The total number of samples is the number of labels.\n",
    "        return len(self.labels)\n",
    "\n",
    "    def retrieve_image(self, idx):\n",
    "        \"\"\"\n",
    "        Loads and validates a single image from disk.\n",
    "\n",
    "        Args:\n",
    "            idx (int): The index of the image to load.\n",
    "\n",
    "        Returns:\n",
    "            PIL.Image.Image: The validated and loaded image object.\n",
    "        \"\"\"\n",
    "        # Construct the image filename based on the index.\n",
    "        img_name = f\"image_{idx+1:05d}.jpg\"\n",
    "        # Construct the full path to the image file.\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "        # Open the image file to check its integrity without loading fully.\n",
    "        with Image.open(img_path) as img:\n",
    "            # Perform a quick verification of the file's structure.\n",
    "            img.verify()\n",
    "        # Re-open the image file after successful verification.\n",
    "        image = Image.open(img_path)\n",
    "        # Fully load the image data into memory.\n",
    "        image.load()\n",
    "        # Check if the image dimensions are below a minimum threshold.\n",
    "        if image.size[0] < 32 or image.size[1] < 32:\n",
    "            # Raise an error for images that are too small.\n",
    "            raise ValueError(f\"Image too small: {image.size}\")\n",
    "        # Check if the image is not in the RGB color mode.\n",
    "        if image.mode != \"RGB\":\n",
    "            # Convert the image to RGB.\n",
    "            image = image.convert(\"RGB\")\n",
    "        # Return the fully loaded and validated image.\n",
    "        return image\n",
    "\n",
    "    def load_and_correct_labels(self):\n",
    "        \"\"\"\n",
    "        Loads labels from a .mat file and adjusts them.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: An array of zero-indexed integer labels.\n",
    "        \"\"\"\n",
    "        # Load the MATLAB file containing the labels.\n",
    "        self.labels_mat = scipy.io.loadmat(\n",
    "            os.path.join(self.root_dir, \"imagelabels.mat\")\n",
    "        )\n",
    "        # Extract the labels array and correct for zero-based indexing.\n",
    "        labels = self.labels_mat[\"labels\"][0] - 1\n",
    "        # Truncate the dataset to the first 10 labels for quick testing.\n",
    "        labels = labels[:10]\n",
    "        # Return the processed labels.\n",
    "        return labels\n",
    "\n",
    "    def log_error(self, idx, e):\n",
    "        \"\"\"\n",
    "        Records the details of an error encountered during data loading.\n",
    "\n",
    "        Args:\n",
    "            idx (int): The index of the problematic sample.\n",
    "            e (Exception): The exception object that was raised.\n",
    "        \"\"\"\n",
    "        # Construct the filename of the problematic image.\n",
    "        img_name = f\"image_{idx + 1:05d}.jpg\"\n",
    "        # Construct the full path to the image file.\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "        # Append a dictionary with error details to the log.\n",
    "        self.error_logs.append(\n",
    "            {\n",
    "                \"index\": idx,\n",
    "                \"error\": str(e),\n",
    "                \"path\": img_path if \"img_path\" in locals() else \"unknown\",\n",
    "            }\n",
    "        )\n",
    "        # Print a warning to the console about the skipped image.\n",
    "        print(f\"Warning: Skipping corrupted image {idx}: {e}\")\n",
    "\n",
    "    def get_error_summary(self):\n",
    "        \"\"\"\n",
    "        Prints a summary of all errors encountered during dataset processing.\n",
    "        \"\"\"\n",
    "        # Check if the error log is empty.\n",
    "        if not self.error_logs:\n",
    "            # Print a message indicating the dataset is clean.\n",
    "            print(\"No errors encountered - dataset is clean!\")\n",
    "        else:\n",
    "            # Print the total number of problematic images found.\n",
    "            print(f\"\\nEncountered {len(self.error_logs)} problematic images:\")\n",
    "            # Iterate through the first few logged errors.\n",
    "            for error in self.error_logs[:5]:\n",
    "                # Print the details of an individual error.\n",
    "                print(f\"  Index {error['index']}: {error['error']}\")\n",
    "            # Check if there are more errors than were displayed.\n",
    "            if len(self.error_logs) > 5:\n",
    "                # Print a summary of the remaining errors.\n",
    "                print(f\"  ... and {len(self.error_logs) - 5} more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7f584f",
   "metadata": {},
   "source": [
    "The Oxford Flowers 102 dataset is clean, but to illustrate robustness techniques, a subset of images in the dataset have been intentionally corrupted. The files of this subset are in the folder `./corrupted_flower_data`.\n",
    "\n",
    "You can create an instance of the `RobustFlowerDataset` class, passing the path to the corrupted dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da25c080",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the path to the directory containing the corrupted dataset.\n",
    "corrupted_dataset_path = './corrupted_flower_data'\n",
    "\n",
    "# Initialize the robust dataset handler with the path to the corrupted data.\n",
    "robust_dataset = RobustFlowerDataset(corrupted_dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4446aab7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the index to a known corrupted image. Image 2 is tiny\n",
    "idx = 2\n",
    "\n",
    "# Attempt to retrieve the image; the robust dataset will skip the bad one and return the next.\n",
    "img, label = robust_dataset[idx]\n",
    "\n",
    "# Plot the retrieved image, which should be the one following the corrupted one.\n",
    "helper_utils.plot_img(img)\n",
    "\n",
    "# Explicitly retrieve the next image in the sequence to verify.\n",
    "next_img, next_label = robust_dataset[idx + 1]\n",
    "\n",
    "# Plot the next image; it should be identical to the one above.\n",
    "helper_utils.plot_img(next_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c6af73-4907-42cd-ad34-8e30a3aca39b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the index to a known grayscale image.\n",
    "# Image 4 is corrupted (grayscale)\n",
    "idx = 4\n",
    "\n",
    "# Reconstruct the path to the original image file.\n",
    "original_img_path = os.path.join(robust_dataset.img_dir, f\"image_{idx + 1:05d}.jpg\")\n",
    "# Open the original image directly to check its mode before correction.\n",
    "original_img = Image.open(original_img_path)\n",
    "# Print the mode of the original, uncorrected image.\n",
    "print(f\"Mode of the original image file: {original_img.mode}\")  # Prints 'L' for 8-bit grayscale. A standard color image would be 'RGB'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7709c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Retrieve the image; the robust loader should automatically convert it to RGB.\n",
    "img, label = robust_dataset[idx]\n",
    "\n",
    "# Plot the image to visually confirm it's now in color.\n",
    "helper_utils.plot_img(img)\n",
    "\n",
    "# Print the image's mode to confirm it has been corrected to 'RGB'.\n",
    "print(f\"Mode of the corrected image: {img.mode}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb48623",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the index to a known corrupted or unreadable image.\n",
    "idx = 6\n",
    "\n",
    "# Attempt to retrieve the image; the robust loader should skip the corrupted file and return the next one.\n",
    "robust_img = robust_dataset[idx][0]\n",
    "\n",
    "# Plot the retrieved image, which should be the sample from the next index (7).\n",
    "helper_utils.plot_img(robust_img)\n",
    "\n",
    "# Explicitly retrieve the next image in the sequence to verify the fallback logic.\n",
    "# Check next image to ensure it's correct\n",
    "next_img, next_label = robust_dataset[idx + 1]\n",
    "\n",
    "# Plot the next image; it should be identical to the one above, confirming the skip.\n",
    "helper_utils.plot_img(next_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff75c89",
   "metadata": {},
   "source": [
    "You can see that images 2, 4 and 6 are corrupted in different ways, but the robust dataset class is able to handle those cases accordingly.\n",
    "You can get a summary of the errors encountered during data loading by inspecting the `error_logs` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd32100e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Display the summary of any corrupted or problematic images found during loading.\n",
    "robust_dataset.get_error_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c31cba7",
   "metadata": {},
   "source": [
    "### Tracking Errors\n",
    "\n",
    "In real-world deep learning projects, data pipelines need to be robust not only to corrupted files but also to subtle issues that can silently degrade model performance.\n",
    "Beyond handling exceptions, it’s important to systematically track and analyze the errors and anomalies that occur during data loading and preprocessing.\n",
    "\n",
    "This final section introduces practical strategies for error tracking within your dataset pipeline.\n",
    "You’ll learn how to log problematic samples, monitor which images are accessed (and how often), and review error summaries after training.\n",
    "These techniques help ensure that your pipeline is resilient, transparent, and production-ready, allowing you to detect and resolve data quality issues before they affect your model’s results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3544bd",
   "metadata": {},
   "source": [
    "The `MonitoredDataset` class extends `RobustFlowerDataset` to add monitoring features for dataset access and loading performance.\n",
    "\n",
    "- **Access Tracking**: Each time an image is loaded through `__getitem__`, the class increments a counter for that image index in `self.access_counts`. This allows you to track how often each sample is accessed during training or evaluation.\n",
    "\n",
    "- **Load Time Measurement**: This class records the time taken to load each image and stores these values in `self.load_times`. If loading an image takes longer than 1 second, a warning is printed with the image index and load time. This helps identify slow-loading samples that may affect training performance.\n",
    "\n",
    "- **Statistics Reporting**: The `print_stats()` method provides a summary of dataset usage, including:\n",
    "    - Total number of images in the dataset\n",
    "    - Number of unique images accessed\n",
    "    - Number of errors encountered (inherited from the parent class)\n",
    "    - Average and maximum image load times\n",
    "    - A warning if any images were never accessed, with examples\n",
    "\n",
    "These monitoring capabilities help you spot bottlenecks, diagnose errors, and ensure the reliability of your data pipeline—making it easier to debug and optimize dataset handling in deep learning workflows.\n",
    "\n",
    "The MonitoredDataset class extends RobustFlowerDataset to add monitoring features for dataset access and loading performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbedb1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MonitoredDataset(RobustFlowerDataset):\n",
    "    \"\"\"\n",
    "    Extends a robust dataset class to add performance monitoring.\n",
    "\n",
    "    This class tracks metrics such as how frequently each image is accessed,\n",
    "    how long each access takes, and which images are never loaded. It provides\n",
    "    a summary of these statistics to help diagnose data pipeline issues.\n",
    "    \"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Initializes the monitored dataset object.\n",
    "\n",
    "        Args:\n",
    "            *args: Variable length argument list passed to the parent class.\n",
    "            **kwargs: Arbitrary keyword arguments passed to the parent class.\n",
    "        \"\"\"\n",
    "        # Initialize the parent class with all provided arguments.\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # Initialize a dictionary to count how many times each index is accessed.\n",
    "        self.access_counts = {}\n",
    "        # Initialize a list to store the load time for each access.\n",
    "        self.load_times = []\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves a sample while monitoring access counts and load times.\n",
    "\n",
    "        Args:\n",
    "            idx (int): The index of the sample to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            tuple: The data sample (e.g., image and label) from the parent class.\n",
    "        \"\"\"\n",
    "        # Import the time module for timing operations.\n",
    "        import time\n",
    "        # Record the start time of the operation.\n",
    "        start_time = time.time()\n",
    "        # Increment the access count for the given index.\n",
    "        self.access_counts[idx] = self.access_counts.get(idx, 0) + 1\n",
    "        # Call the parent class's method to load the data.\n",
    "        result = super().__getitem__(idx)\n",
    "        # Calculate the total time taken to load the sample.\n",
    "        load_time = time.time() - start_time\n",
    "        # Append the calculated load time to the list.\n",
    "        self.load_times.append(load_time)\n",
    "        # Check if the load time exceeds a certain threshold.\n",
    "        if load_time > 1.0:\n",
    "            # Print a warning if a slow load time is detected.\n",
    "            print(f\"⚠️ Slow load: Image {idx} took {load_time:.2f}s\")\n",
    "        # Return the loaded sample from the parent class.\n",
    "        return result\n",
    "\n",
    "    def print_stats(self):\n",
    "        \"\"\"\n",
    "        Prints a summary of the dataset's access statistics and performance.\n",
    "        \"\"\"\n",
    "        # Print a header for the statistics report.\n",
    "        print(\"\\n=== Pipeline Statistics ===\")\n",
    "        # Display the total number of images in the dataset.\n",
    "        print(f\"Total images: {len(self)}\")\n",
    "        # Display the number of unique images that were accessed.\n",
    "        print(f\"Unique images accessed: {len(self.access_counts)}\")\n",
    "        # Display the total number of errors logged by the parent class.\n",
    "        print(f\"Errors encountered: {len(self.error_logs)}\")\n",
    "        # Check if any load times have been recorded.\n",
    "        if self.load_times:\n",
    "            # Calculate the average load time.\n",
    "            avg_time = sum(self.load_times) / len(self.load_times)\n",
    "            # Find the maximum (slowest) load time.\n",
    "            max_time = max(self.load_times)\n",
    "            # Print the average load time in milliseconds.\n",
    "            print(f\"Average load time: {avg_time*1000:.1f} ms\")\n",
    "            # Print the slowest load time in milliseconds.\n",
    "            print(f\"Slowest load: {max_time*1000:.1f} ms\")\n",
    "        # Create a set of all possible indices in the dataset.\n",
    "        all_indices = set(range(len(self)))\n",
    "        # Create a set of all indices that were actually accessed.\n",
    "        accessed_indices = set(self.access_counts.keys())\n",
    "        # Find the set of indices that were never accessed.\n",
    "        never_accessed = all_indices - accessed_indices\n",
    "        # Check if there are any images that were never loaded.\n",
    "        if never_accessed:\n",
    "            # Print a warning message with the count of never-accessed images.\n",
    "            print(f\"\\n⚠️ WARNING: {len(never_accessed)} images were never loaded!\")\n",
    "            # Show a few examples of the indices that were never accessed.\n",
    "            print(f\"   Examples: {list(never_accessed)[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77efe078",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize the monitored dataset with the path to the potentially corrupted data.\n",
    "monitored_dataset = MonitoredDataset(corrupted_dataset_path)\n",
    "\n",
    "# Loop through every index in the dataset to simulate a full pass.\n",
    "# Iterate through the dataset to trigger monitoring\n",
    "for idx in range(len(monitored_dataset)):\n",
    "    # Access the sample at the current index to trigger the monitoring and error-handling logic.\n",
    "    img, label = monitored_dataset[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907f7393",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print the statistics\n",
    "monitored_dataset.print_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b048126",
   "metadata": {},
   "source": [
    "The `print_stats` method provides a comprehensive overview of the dataset's usage and any issues encountered during data loading.\n",
    "This is a good practice to implement in your dataset classes, as it helps you monitor the quality of your data and the performance of your data pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e86dfe",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Congratulations! You’ve now built a complete, production-ready data pipeline in PyTorch.\n",
    "In this lab, you moved beyond working with perfectly curated data and learned how to manage the real-world complexities of data handling—from initial access to efficient batching.\n",
    "\n",
    "You saw how to organize a messy dataset by creating a custom **`Dataset`** class to load images and labels from separate files. You applied essential preprocessing steps, including **transformations** to normalize your data and **data augmentation** to make your future model more robust.\n",
    "\n",
    "You then used  **`DataLoader`** to prepare your data by batching and shuffling it for training. Finally, you made your pipeline more reliable by implementing error handling and performance monitoring, ensuring that your model’s training process is both stable and efficient.\n",
    "\n",
    "You’ve built a strong foundation in data management that will support everything you do next."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
